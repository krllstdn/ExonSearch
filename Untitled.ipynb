{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac00969-a078-4695-b4ed-19eedbddf6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 10:32:39.740274: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 10:32:39.741169: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 10:32:39.744034: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-03 10:32:39.806679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746261159.866213 3386696 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746261159.889418 3386696 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746261159.951815 3386696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746261159.951830 3386696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746261159.951831 3386696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746261159.951832 3386696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 10:32:39.955267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kstadniuk/ExonSearch/utils.py:36: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gtf = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import parse_gtf\n",
    "\n",
    "# Configuration\n",
    "WINDOW_SIZE = 101\n",
    "CHROMOSOME_PREFIX = \"\"  # \"chr\" if needed to match FASTA\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.0005\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# 1. Preprocess All Chromosomes First =========================================\n",
    "chromosome_names = [str(i) for i in range(1, 23)] + [\"X\", \"Y\"]\n",
    "\n",
    "# Global gene split (all chromosomes)\n",
    "gene_groups = parse_gtf(\"data/Homo_sapiens.GRCh38.113.chr.gtf.gz\") \n",
    "gene_ids = list(gene_groups.groups.keys())\n",
    "train_genes, test_genes = train_test_split(gene_ids, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513b2556-6fd4-4c97-b7b5-024d0016e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, kernel_size=10, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(128, kernel_size=5, activation='relu')(x)\n",
    "\n",
    "    # Attention layer\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    #     initial_learning_rate=0.0003,\n",
    "    #     decay_steps=10000\n",
    "    # )\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5e7046-1fb3-4b02-b3b9-36a9959fbcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.6014 - loss: 0.6524 - precision_4: 0.6053 - recall_4: 0.5710"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 130ms/step - accuracy: 0.6014 - loss: 0.6524 - precision_4: 0.6054 - recall_4: 0.5711 - val_accuracy: 0.7051 - val_loss: 0.5725 - val_precision_4: 0.7320 - val_recall_4: 0.6470\n",
      "Epoch 2/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7086 - loss: 0.5727 - precision_4: 0.7196 - recall_4: 0.6818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 130ms/step - accuracy: 0.7086 - loss: 0.5727 - precision_4: 0.7196 - recall_4: 0.6818 - val_accuracy: 0.7184 - val_loss: 0.5556 - val_precision_4: 0.7572 - val_recall_4: 0.6429\n",
      "Epoch 3/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7202 - loss: 0.5597 - precision_4: 0.7295 - recall_4: 0.6993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 133ms/step - accuracy: 0.7202 - loss: 0.5597 - precision_4: 0.7295 - recall_4: 0.6993 - val_accuracy: 0.7274 - val_loss: 0.5458 - val_precision_4: 0.7482 - val_recall_4: 0.6855\n",
      "Epoch 4/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7311 - loss: 0.5469 - precision_4: 0.7416 - recall_4: 0.7074"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 124ms/step - accuracy: 0.7311 - loss: 0.5469 - precision_4: 0.7416 - recall_4: 0.7074 - val_accuracy: 0.7269 - val_loss: 0.5456 - val_precision_4: 0.7610 - val_recall_4: 0.6615\n",
      "Epoch 5/100\n",
      "\u001b[1m 351/1000\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 111ms/step - accuracy: 0.7302 - loss: 0.5428 - precision_4: 0.7439 - recall_4: 0.6962"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kstadniuk/ExonSearch/venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 73ms/step - accuracy: 0.7300 - loss: 0.5444 - precision_4: 0.7426 - recall_4: 0.6964 - val_accuracy: 0.7237 - val_loss: 0.5491 - val_precision_4: 0.7699 - val_recall_4: 0.6381\n",
      "Epoch 6/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 150ms/step - accuracy: 0.7343 - loss: 0.5408 - precision_4: 0.7461 - recall_4: 0.7043 - val_accuracy: 0.7226 - val_loss: 0.5522 - val_precision_4: 0.6911 - val_recall_4: 0.8053\n",
      "Epoch 7/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7366 - loss: 0.5380 - precision_4: 0.7482 - recall_4: 0.7177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 151ms/step - accuracy: 0.7366 - loss: 0.5380 - precision_4: 0.7482 - recall_4: 0.7177 - val_accuracy: 0.7316 - val_loss: 0.5409 - val_precision_4: 0.7752 - val_recall_4: 0.6525\n",
      "Epoch 8/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7408 - loss: 0.5336 - precision_4: 0.7538 - recall_4: 0.7169"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 156ms/step - accuracy: 0.7408 - loss: 0.5336 - precision_4: 0.7538 - recall_4: 0.7169 - val_accuracy: 0.7338 - val_loss: 0.5374 - val_precision_4: 0.7469 - val_recall_4: 0.7075\n",
      "Epoch 9/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 149ms/step - accuracy: 0.7436 - loss: 0.5308 - precision_4: 0.7552 - recall_4: 0.7183 - val_accuracy: 0.7333 - val_loss: 0.5384 - val_precision_4: 0.7556 - val_recall_4: 0.6897\n",
      "Epoch 10/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 71ms/step - accuracy: 0.7487 - loss: 0.5238 - precision_4: 0.7628 - recall_4: 0.7237 - val_accuracy: 0.7315 - val_loss: 0.5451 - val_precision_4: 0.7143 - val_recall_4: 0.7716\n",
      "Epoch 11/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7459 - loss: 0.5267 - precision_4: 0.7604 - recall_4: 0.7190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 143ms/step - accuracy: 0.7459 - loss: 0.5267 - precision_4: 0.7604 - recall_4: 0.7190 - val_accuracy: 0.7346 - val_loss: 0.5372 - val_precision_4: 0.7292 - val_recall_4: 0.7464\n",
      "Epoch 12/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7426 - loss: 0.5288 - precision_4: 0.7561 - recall_4: 0.7174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 144ms/step - accuracy: 0.7426 - loss: 0.5288 - precision_4: 0.7561 - recall_4: 0.7174 - val_accuracy: 0.7359 - val_loss: 0.5343 - val_precision_4: 0.7622 - val_recall_4: 0.6858\n",
      "Epoch 13/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 147ms/step - accuracy: 0.7461 - loss: 0.5258 - precision_4: 0.7572 - recall_4: 0.7210 - val_accuracy: 0.7364 - val_loss: 0.5350 - val_precision_4: 0.7573 - val_recall_4: 0.6958\n",
      "Epoch 14/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.7469 - loss: 0.5228 - precision_4: 0.7612 - recall_4: 0.7182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 145ms/step - accuracy: 0.7469 - loss: 0.5228 - precision_4: 0.7612 - recall_4: 0.7182 - val_accuracy: 0.7371 - val_loss: 0.5334 - val_precision_4: 0.7347 - val_recall_4: 0.7421\n",
      "Epoch 15/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 71ms/step - accuracy: 0.7527 - loss: 0.5174 - precision_4: 0.7641 - recall_4: 0.7336 - val_accuracy: 0.7373 - val_loss: 0.5337 - val_precision_4: 0.7633 - val_recall_4: 0.6878\n",
      "Epoch 16/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 119ms/step - accuracy: 0.7517 - loss: 0.5184 - precision_4: 0.7650 - recall_4: 0.7231 - val_accuracy: 0.7260 - val_loss: 0.5518 - val_precision_4: 0.6973 - val_recall_4: 0.7989\n",
      "Epoch 17/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 114ms/step - accuracy: 0.7559 - loss: 0.5130 - precision_4: 0.7687 - recall_4: 0.7321 - val_accuracy: 0.7368 - val_loss: 0.5339 - val_precision_4: 0.7755 - val_recall_4: 0.6664\n",
      "Epoch 18/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.7570 - loss: 0.5096 - precision_4: 0.7692 - recall_4: 0.7334 - val_accuracy: 0.7344 - val_loss: 0.5355 - val_precision_4: 0.7699 - val_recall_4: 0.6687\n",
      "Epoch 19/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.7561 - loss: 0.5115 - precision_4: 0.7712 - recall_4: 0.7293 - val_accuracy: 0.7343 - val_loss: 0.5395 - val_precision_4: 0.7260 - val_recall_4: 0.7527\n",
      "Epoch 20/100\n",
      "\u001b[1m 351/1000\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 90ms/step - accuracy: 0.7526 - loss: 0.5144 - precision_4: 0.7629 - recall_4: 0.7312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57ms/step - accuracy: 0.7545 - loss: 0.5133 - precision_4: 0.7665 - recall_4: 0.7318 - val_accuracy: 0.7390 - val_loss: 0.5311 - val_precision_4: 0.7569 - val_recall_4: 0.7042\n",
      "Epoch 21/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.7532 - loss: 0.5139 - precision_4: 0.7673 - recall_4: 0.7294 - val_accuracy: 0.7387 - val_loss: 0.5314 - val_precision_4: 0.7363 - val_recall_4: 0.7437\n",
      "Epoch 22/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.7569 - loss: 0.5096 - precision_4: 0.7704 - recall_4: 0.7330 - val_accuracy: 0.7362 - val_loss: 0.5359 - val_precision_4: 0.7594 - val_recall_4: 0.6915\n",
      "Epoch 23/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.7614 - loss: 0.5040 - precision_4: 0.7761 - recall_4: 0.7363 - val_accuracy: 0.7351 - val_loss: 0.5370 - val_precision_4: 0.7295 - val_recall_4: 0.7473\n",
      "Epoch 24/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 113ms/step - accuracy: 0.7578 - loss: 0.5081 - precision_4: 0.7706 - recall_4: 0.7315 - val_accuracy: 0.7378 - val_loss: 0.5329 - val_precision_4: 0.7558 - val_recall_4: 0.7027\n",
      "Epoch 25/100\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 56ms/step - accuracy: 0.7605 - loss: 0.5051 - precision_4: 0.7752 - recall_4: 0.7399 - val_accuracy: 0.7297 - val_loss: 0.5451 - val_precision_4: 0.7051 - val_recall_4: 0.7897\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "class ChromosomeDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, chr_files, batch_size=256):\n",
    "        self.chr_files = chr_files\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(sum(\n",
    "            h5py.File(f, 'r')['X_train'].shape[0] for f in self.chr_files\n",
    "        ) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get random chromosome\n",
    "        chr_file = np.random.choice(self.chr_files)\n",
    "        \n",
    "        with h5py.File(chr_file, 'r') as f:\n",
    "            X = f['X_train']\n",
    "            y = f['y_train']\n",
    "            \n",
    "            # Get random batch from chromosome\n",
    "            start = np.random.randint(0, len(X) - self.batch_size)\n",
    "            return X[start:start+self.batch_size], y[start:start+self.batch_size]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.chr_files)\n",
    "\n",
    "class ValidationDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, chr_files, batch_size=512):\n",
    "        self.chr_files = chr_files\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        total_samples = sum(\n",
    "            h5py.File(f, 'r')['X_test'].shape[0] for f in self.chr_files\n",
    "        )\n",
    "        return int(np.ceil(total_samples / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load one chromosome at a time\n",
    "        chr_file = self.chr_files[index % len(self.chr_files)]\n",
    "        \n",
    "        with h5py.File(chr_file, 'r') as f:\n",
    "            X = f['X_test'][:]\n",
    "            y = f['y_test'][:]\n",
    "            \n",
    "            # Return full chromosome test set (batched automatically)\n",
    "            return X, y\n",
    "\n",
    "# Initialize\n",
    "val_generator = ValidationDataGenerator(\n",
    "    [f'data/bin/train_test_data{n}.h5' for n in chromosome_names]\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "all_chr_files = [f'data/bin/train_test_data{n}.h5' for n in chromosome_names]\n",
    "model = build_model((WINDOW_SIZE, 4))\n",
    "\n",
    "# Training with chromosome rotation\n",
    "history = model.fit(\n",
    "    ChromosomeDataGenerator(all_chr_files),\n",
    "    epochs=100,\n",
    "    steps_per_epoch=1000,  # Adjust based on total data size\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=20, #len(val_generator),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fdc2b01-10e7-4d82-8982-bf427d8b4a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_generator),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36194643-d98e-4ff8-ab5d-de031b21a78b",
   "metadata": {},
   "source": [
    "using the batches from all chromosomes and bigger batch size improved the accuracy 0.745->0.7605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff47508c-14f5-48f6-946d-69a34e758360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading gtf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kstadniuk/ExonSearch/utils.py:36: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gtf = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting_genes...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading gtf...\") \n",
    "gene_groups = parse_gtf(\"data/Homo_sapiens.GRCh38.113.chr.gtf.gz\") # must be outside loop\n",
    "\n",
    "# Split genes\n",
    "print(\"splitting_genes...\")\n",
    "gene_ids = list(gene_groups.groups.keys())\n",
    "train_genes, test_genes = train_test_split(\n",
    "    gene_ids, test_size=0.2, random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a61503-0626-4b18-be24-6d47656eaa9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"H5F.c\", line 836, in H5Fopen\n    unable to synchronously open file\n  File \"H5F.c\", line 796, in H5F__open_api_common\n    unable to open file\n  File \"H5VLcallback.c\", line 3863, in H5VL_file_open\n    open failed\n  File \"H5VLcallback.c\", line 3675, in H5VL__file_open\n    open failed\n  File \"H5VLnative_file.c\", line 128, in H5VL__native_file_open\n    unable to open file\n  File \"H5Fint.c\", line 1910, in H5F_open\n    unable to lock the file\n  File \"H5FD.c\", line 2412, in H5FD_lock\n    driver lock request failed\n  File \"H5FDsec2.c\", line 941, in H5FD__sec2_lock\n    unable to lock file, errno = 11, error message = 'Resource temporarily unavailable'\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'genes.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHDF5ExtError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m f.create_dataset(\u001b[33m'\u001b[39m\u001b[33mtrain_genes\u001b[39m\u001b[33m'\u001b[39m, data=train_genes)\n\u001b[32m      5\u001b[39m f.create_dataset(\u001b[33m'\u001b[39m\u001b[33mtest_genes\u001b[39m\u001b[33m'\u001b[39m, data=test_genes)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mgene_groups\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgenes.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgene_df\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/pandas/core/generic.py:2855\u001b[39m, in \u001b[36mNDFrame.to_hdf\u001b[39m\u001b[34m(self, path_or_buf, key, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[39m\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pytables\n\u001b[32m   2853\u001b[39m \u001b[38;5;66;03m# Argument 3 to \"to_hdf\" has incompatible type \"NDFrame\"; expected\u001b[39;00m\n\u001b[32m   2854\u001b[39m \u001b[38;5;66;03m# \"Union[DataFrame, Series]\" [arg-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2855\u001b[39m \u001b[43mpytables\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_hdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2858\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomplib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2863\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2869\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/pandas/io/pytables.py:308\u001b[39m, in \u001b[36mto_hdf\u001b[39m\u001b[34m(path_or_buf, key, value, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[39m\n\u001b[32m    306\u001b[39m path_or_buf = stringify_path(path_or_buf)\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplib\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m store:\n\u001b[32m    311\u001b[39m         f(store)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/pandas/io/pytables.py:585\u001b[39m, in \u001b[36mHDFStore.__init__\u001b[39m\u001b[34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m._fletcher32 = fletcher32\n\u001b[32m    584\u001b[39m \u001b[38;5;28mself\u001b[39m._filters = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/pandas/io/pytables.py:745\u001b[39m, in \u001b[36mHDFStore.open\u001b[39m\u001b[34m(self, mode, **kwargs)\u001b[39m\n\u001b[32m    739\u001b[39m     msg = (\n\u001b[32m    740\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot open HDF5 file, which is already opened, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    741\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33meven in read-only mode.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    742\u001b[39m     )\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m \u001b[38;5;28mself\u001b[39m._handle = \u001b[43mtables\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/tables/file.py:325\u001b[39m, in \u001b[36mopen_file\u001b[39m\u001b[34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is already opened.  Please \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mclose it before reopening in write mode.\u001b[39m\u001b[33m\"\u001b[39m % filename\n\u001b[32m    322\u001b[39m             )\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_uep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/tables/file.py:811\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.params = params\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_g_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[32m    814\u001b[39m new = \u001b[38;5;28mself\u001b[39m._v_new\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ExonSearch/venv/lib/python3.12/site-packages/tables/hdf5extension.pyx:617\u001b[39m, in \u001b[36mtables.hdf5extension.File._g_new\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mHDF5ExtError\u001b[39m: HDF5 error back trace\n\n  File \"H5F.c\", line 836, in H5Fopen\n    unable to synchronously open file\n  File \"H5F.c\", line 796, in H5F__open_api_common\n    unable to open file\n  File \"H5VLcallback.c\", line 3863, in H5VL_file_open\n    open failed\n  File \"H5VLcallback.c\", line 3675, in H5VL__file_open\n    open failed\n  File \"H5VLnative_file.c\", line 128, in H5VL__native_file_open\n    unable to open file\n  File \"H5Fint.c\", line 1910, in H5F_open\n    unable to lock the file\n  File \"H5FD.c\", line 2412, in H5FD_lock\n    driver lock request failed\n  File \"H5FDsec2.c\", line 941, in H5FD__sec2_lock\n    unable to lock file, errno = 11, error message = 'Resource temporarily unavailable'\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'genes.h5'"
     ]
    }
   ],
   "source": [
    "with h5py.File(f'genes.h5', 'w') as f:\n",
    "    # Save the tuple as a dataset\n",
    "    # f.create_dataset('gene_groups', data=gene_groups)\n",
    "    f.create_dataset('train_genes', data=train_genes)\n",
    "    f.create_dataset('test_genes', data=test_genes)\n",
    "    gene_groups.obj.to_hdf('genes.h5', key='gene_df', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66457d8-ea7f-4f60-a1e1-0a51d4b7852f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
